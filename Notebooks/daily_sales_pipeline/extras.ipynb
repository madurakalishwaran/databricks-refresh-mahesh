{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "95a21b47-f5a0-4a4e-a60d-755038e2f503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"extras\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "af6e1884-dfd4-4ce0-a64e-fbb4bd4e44af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create metrics table 'sales_pipeline_metrics'\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"run_id\", StringType(), nullable=False),\n",
    "        StructField(\"run_timestamp\", TimestampType(), nullable=False),\n",
    "        StructField(\"bronze_row_count\", LongType(), nullable=False),\n",
    "        StructField(\"silver_valid_count\", LongType(), nullable=False),\n",
    "        StructField(\"silver_quarantine_count\", LongType(), nullable=False),\n",
    "        StructField(\"fact_rows_inserted\", LongType(), nullable=False),\n",
    "        StructField(\"fact_rows_updated\", LongType(), nullable=False),\n",
    "        StructField(\"run_status\", StringType(), nullable=False)\n",
    "    ]\n",
    ")\n",
    "\n",
    "metrics_table = spark.createDataFrame([], schema)\n",
    "metrics_table.printSchema()\n",
    "\n",
    "metrics_table.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_pipeline_metrics\")\n",
    "\n",
    "if spark.catalog.tableExists(\"sales_pipeline_metrics\"):\n",
    "    print(\"Table exists.\")\n",
    "else:\n",
    "    print(\"Table is not created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dc120985-a13c-49f7-bf82-fa1f1f1ede74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bulk remove tables\n",
    "tables_to_drop = [\"sales_bronze\", \"sales_silver\", \"sales_quarantine\", \"sales_fact\"]\n",
    "\n",
    "for t in tables_to_drop:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ff6e440f-be73-4ee4-b77f-24c3ae6e1a41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create file proccessed table\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"file_name\", StringType(), nullable=False),\n",
    "        StructField(\"processed_timestamp\", TimestampType(), nullable=False)\n",
    "    ]\n",
    ")\n",
    "\n",
    "file_processed_table = spark.createDataFrame([], schema)\n",
    "file_processed_table.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_processed_files\")\n",
    "\n",
    "if spark.catalog.tableExists(\"sales_processed_files\"):\n",
    "    print(\"Table exists.\")\n",
    "else:\n",
    "    print(\"Table is not created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4fdc0ae9-5732-4dde-9d0a-4f4d5d2c74df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# keep in here temporarily\n",
    "'''# Initialize variables\n",
    "\n",
    "SOURCE_PATH = \"/Volumes/workspace/default/my_datas/daily_sales/sales_1000.csv\"\n",
    "BRONZE_TABLE = \"sales_bronze\"\n",
    "SILVER_TABLE = \"sales_silver\"\n",
    "QUARANTINE_TABLE = \"sales_quarantine\"\n",
    "FACT_TABLE = \"sales_fact\"\n",
    "\n",
    "run_metadata = {\n",
    "    \"run_id\": str(uuid.uuid4()),\n",
    "    \"run_timestamp\": datetime.datetime.now(),\n",
    "    \"bronze_row_count\": 0,\n",
    "    \"silver_valid_count\": 0,\n",
    "    \"silver_quarantine_count\": 0,\n",
    "    \"fact_rows_inserted\": 0,\n",
    "    \"fact_rows_updated\": 0,\n",
    "    \"run_status\": \"success\"\n",
    "}\n",
    "\n",
    "def write_pipeline_metrics(get_data):\n",
    "    \n",
    "    target_schema = spark.table('sales_pipeline_metrics').schema\n",
    "    metrics_df = spark.createDataFrame([get_data], schema=target_schema)\n",
    "    metrics_df.write.format('delta').mode(\"append\").saveAsTable('sales_pipeline_metrics')\n",
    "    print(\"Metrics appended successfully\")\n",
    "\n",
    "try:\n",
    "    # Read CSV\n",
    "    bronze_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(SOURCE_PATH)\n",
    "\n",
    "    # clean column names\n",
    "    bronze_df = bronze_df.select([col(c).alias(c.replace(\" \", \"_\")) for c in bronze_df.columns])\n",
    "\n",
    "    # write to delta\n",
    "    bronze_df.write.format('delta').mode(\"overwrite\").saveAsTable(BRONZE_TABLE)\n",
    "\n",
    "    # Phase 2 â€” Basic Data Quality (Silver + Quarantine)\n",
    "\n",
    "    # Read from sales_bronze table\n",
    "    bronze_temp_df = spark.read.table(BRONZE_TABLE)\n",
    "    # Get bronze table count\n",
    "    run_metadata[\"bronze_row_count\"] = spark.table(BRONZE_TABLE).count()\n",
    "\n",
    "    # clean column names - Advanced - using Regex\n",
    "    bronze_temp_df = bronze_temp_df.select([\n",
    "        col(c)\n",
    "        .alias(re.sub(r'[^a-zA-Z0-9]+', '_', c)\n",
    "            .lower()\n",
    "            .strip('_')) \n",
    "        for c in bronze_temp_df.columns\n",
    "    ])\n",
    "\n",
    "    # Casting critical columns\n",
    "    bronze_temp_df = (\n",
    "        bronze_temp_df\n",
    "        .withColumns(\n",
    "            {\n",
    "                \"order_id\": col('order_id').cast('long'),\n",
    "                \"units_sold\": col('units_sold').cast('int'),\n",
    "                \"unit_price\": col('unit_price').cast('double') \n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Business conditions to filter rows\n",
    "    invalid_condition = (\n",
    "        (col('order_id').isNull()) | \n",
    "        (col('units_sold') <= 0) | \n",
    "        (col('unit_price') <= 0)\n",
    "    )\n",
    "\n",
    "    # Retrieve valid & invalid rows based on Business conditions\n",
    "    bronze_valid_df = bronze_temp_df.filter(~invalid_condition)\n",
    "    bronze_invalid_df = bronze_temp_df.filter(invalid_condition)\n",
    "\n",
    "    # Write Valid & Invlaid rows to delta\n",
    "    bronze_valid_df.write.format('delta').mode(\"overwrite\").saveAsTable(SILVER_TABLE)\n",
    "    bronze_invalid_df.write.format('delta').mode(\"overwrite\").saveAsTable(QUARANTINE_TABLE)\n",
    "\n",
    "    # Get Silver_valid and Silver_invalid counts\n",
    "    run_metadata[\"silver_valid_count\"] = spark.table(SILVER_TABLE).count()\n",
    "    run_metadata[\"silver_quarantine_count\"] = spark.table(QUARANTINE_TABLE).count()\n",
    "\n",
    "    # Checking counts\n",
    "    print(\"Total_rows: \", run_metadata[\"bronze_row_count\"])\n",
    "    print(\"Valid_rows: \", run_metadata[\"silver_valid_count\"])\n",
    "    print(\"Invalid_rows: \", run_metadata[\"silver_quarantine_count\"])\n",
    "\n",
    "    # Phase 3 - Incremental Loads\n",
    "\n",
    "    # Read from silver valid table\n",
    "    silver_valid_temp = spark.read.table(SILVER_TABLE)\n",
    "\n",
    "    # condtion for incremental load\n",
    "    change_condition = \"\"\"\n",
    "        NOT (t.units_sold <=> s.units_sold) OR\n",
    "        NOT (t.unit_price <=> s.unit_price)\n",
    "    \"\"\"\n",
    "\n",
    "    # Condition for merge\n",
    "    merge_condition = \"t.order_id = s.order_id\"\n",
    "\n",
    "    # Check if table exists an create one and overwrite with silver valid data\n",
    "    if not spark.catalog.tableExists(FACT_TABLE):\n",
    "        print(\"Table does not exist. Creating one..\")\n",
    "        silver_valid_temp.write.format('delta').mode(\"overwrite\").saveAsTable(FACT_TABLE)\n",
    "        print(\"Table created.\")\n",
    "        fact_table_count = spark.table(FACT_TABLE).count()\n",
    "        print(f\"Total No of rows: {fact_table_count}\")\n",
    "\n",
    "    # If table exists, merge the upsert data\n",
    "    else:\n",
    "        print(\"Table exists. Upserting rows..\")\n",
    "\n",
    "        # Create delta table object for sales_fact\n",
    "        delta_sales_fact_temp_df = DeltaTable.forName(spark, FACT_TABLE)\n",
    "        delta_sales_fact_temp_df.alias(\"t\").merge(\n",
    "            silver_valid_temp.alias(\"s\"), \n",
    "            merge_condition\n",
    "            ).whenMatchedUpdateAll(\n",
    "                condition=change_condition\n",
    "                ).whenNotMatchedInsertAll().execute()\n",
    "            \n",
    "        latest_history = delta_sales_fact_temp_df.history(1).collect()[0]\n",
    "        metrics = latest_history[\"operationMetrics\"]\n",
    "        run_metadata[\"fact_rows_inserted\"] = int(metrics.get(\"numTargetRowsInserted\", 0))\n",
    "        run_metadata[\"fact_rows_updated\"] = int(metrics.get(\"numTargetRowsUpdated\", 0))\n",
    "\n",
    "        print(\"new_rows: \", run_metadata[\"fact_rows_inserted\"])\n",
    "        print(\"rows_updated: \", run_metadata[\"fact_rows_updated\"])\n",
    "       \n",
    "\n",
    "except Exception as e:\n",
    "    run_metadata[\"run_status\"] = \"failure\"\n",
    "    raise e\n",
    "\n",
    "finally:\n",
    "    try:\n",
    "        print(\"Attempting to log pipeline metrics...\")\n",
    "        write_pipeline_metrics(run_metadata)\n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL WARNING: Metrics logging failed! Error: {e}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad0ce87-5d69-43a8-84db-7960ef72ced5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769070872051}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--TRUNCATE TABLE sales_pipeline_metrics;\n",
    "--select * from sales_processed_files order by processed_timestamp desc;\n",
    "select * from sales_pipeline_metrics order by run_timestamp desc;\n",
    "--DELETE FROM sales_pipeline_metrics where silver_quarantine_count = '1008';\n",
    "--DELETE FROM sales_processed_files where file_name = 'sales_1000';\n",
    "--show tables;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2f758c0a-8bb9-477e-9c2e-05709da46359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from sales_processed_files;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8670782838204243,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "extras",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
